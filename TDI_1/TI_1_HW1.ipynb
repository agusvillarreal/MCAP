{
    "cells": [
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "# Homework 0\n",
          "\n",
          "**Name:** -- VÃ­ctor Manuel Mariscal Cervantes --\n",
          "\n",
          "**e-mail:** -- victor.mariscal4459@alumnos.udg.mx --"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "# MODULES"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Load modules\n",
          "import numpy as np  # For numerical computations\n",
          "import matplotlib.pyplot as plt  # For plotting results\n",
          "import random  # For generating random numbers\n",
          "\n",
          "# This magic command ensures that plots are displayed inline in the notebook\n",
          "%matplotlib inline"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "# Theory on the Gradient Descent algorithm\n",
          "\n",
          "Gradient Descent is an iterative optimization algorithm used to find the minimum of a function. The algorithm works as follows:\n",
          "\n",
          "1. **Initialization:** Start with an initial guess for the parameters (e.g., a point in the function's domain).\n",
          "\n",
          "2. **Gradient Computation:** Calculate the gradient (i.e., the vector of partial derivatives) of the function at the current point. This gradient indicates the direction of steepest ascent.\n",
          "\n",
          "3. **Parameter Update:** Update the parameters by moving in the opposite direction of the gradient. The update rule is:\n",
          "   \n",
          "   \\[\n",
          "   \\mathbf{x}_{new} = \\mathbf{x}_{current} - \\alpha \\nabla f(\\mathbf{x}_{current})\n",
          "   \\]\n",
          "   \n",
          "   where \\(\\alpha\\) is the learning rate which determines the size of the step.\n",
          "\n",
          "4. **Convergence Check:** Repeat steps 2 and 3 until the change in parameters is less than a predefined tolerance, indicating that a minimum has been reached.\n",
          "\n",
          "In this notebook, we use the Himmelblau function, a well-known function with multiple local minima, to demonstrate gradient descent."
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Function to be optimized\n",
          "\n",
          "def himmelblau(x, y):\n",
          "    \"\"\"\n",
          "    Himmelblau function:\n",
          "    \\( f(x, y) = (x^2 + y - 11)^2 + (x + y^2 - 7)^2 \\)\n",
          "    This function has multiple local minima. The function is used as a performance test for optimization algorithms.\n",
          "    \"\"\"\n",
          "    return (x**2 + y - 11)**2 + (x + y**2 - 7)**2\n",
          "\n",
          "def grad_himmelblau(x, y):\n",
          "    \"\"\"\n",
          "    Computes the gradient of the Himmelblau function with respect to x and y.\n",
          "    The partial derivatives are:\n",
          "    \\( \\frac{\\partial f}{\\partial x} = 4x(x^2 + y - 11) + 2(x + y^2 - 7) \\)\n",
          "    \\( \\frac{\\partial f}{\\partial y} = 2(x^2 + y - 11) + 4y(x + y^2 - 7) \\)\n",
          "    \"\"\"\n",
          "    dfdx = 4 * x * (x**2 + y - 11) + 2 * (x + y**2 - 7)\n",
          "    dfdy = 2 * (x**2 + y - 11) + 4 * y * (x + y**2 - 7)\n",
          "    return np.array([dfdx, dfdy])\n",
          "\n",
          "# The Himmelblau function and its gradient are defined above. These will be used for the optimization process."
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Run gradient descent algorithm\n",
          "\n",
          "# Set parameters for gradient descent\n",
          "learning_rate = 0.001       # Step size for each iteration\n",
          "max_iters = 10000           # Maximum number of iterations allowed\n",
          "tolerance = 1e-6            # Convergence tolerance\n",
          "\n",
          "# Choose a random initial point within the range [-6, 6] for both x and y\n",
          "x0 = random.uniform(-6, 6)\n",
          "y0 = random.uniform(-6, 6)\n",
          "x_current = np.array([x0, y0])\n",
          "print(\"Initial point:\", x_current)\n",
          "\n",
          "# List to store the path taken by the algorithm (for later visualization)\n",
          "path = [x_current.copy()]\n",
          "\n",
          "# Execute the gradient descent loop\n",
          "for i in range(max_iters):\n",
          "    # Compute the gradient at the current point\n",
          "    grad = grad_himmelblau(x_current[0], x_current[1])\n",
          "    \n",
          "    # Update the point by moving in the opposite direction of the gradient\n",
          "    x_new = x_current - learning_rate * grad\n",
          "    \n",
          "    # Append the new point to the path\n",
          "    path.append(x_new.copy())\n",
          "    \n",
          "    # Check for convergence: if the change in position is less than the tolerance, stop iterating\n",
          "    if np.linalg.norm(x_new - x_current) < tolerance:\n",
          "        print(f\"Convergence reached at iteration {i}\")\n",
          "        break\n",
          "    \n",
          "    # Update the current point for the next iteration\n",
          "    x_current = x_new\n",
          "\n",
          "# Convert the list of points to a NumPy array for easier manipulation\n",
          "path = np.array(path)\n",
          "\n",
          "# Output the final position and function value\n",
          "print(\"Final position:\", x_current)\n",
          "print(\"Final function value:\", himmelblau(x_current[0], x_current[1]))"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Plot the results\n",
          "\n",
          "# Create a grid of x and y values to evaluate the Himmelblau function\n",
          "x_vals = np.linspace(-6, 6, 400)\n",
          "y_vals = np.linspace(-6, 6, 400)\n",
          "X, Y = np.meshgrid(x_vals, y_vals)\n",
          "\n",
          "# Evaluate the Himmelblau function on the grid\n",
          "Z = himmelblau(X, Y)\n",
          "\n",
          "# Set up the plot\n",
          "plt.figure(figsize=(10, 6))\n",
          "\n",
          "# Plot the contour lines of the Himmelblau function\n",
          "contour_levels = np.logspace(-0.5, 5, 20)  # Logarithmically spaced levels for better visualization\n",
          "contour_plot = plt.contour(X, Y, Z, levels=contour_levels, norm=plt.LogNorm(), cmap='jet')\n",
          "plt.clabel(contour_plot, inline=True, fontsize=8)\n",
          "\n",
          "# Plot the path taken by the gradient descent algorithm\n",
          "plt.plot(path[:, 0], path[:, 1], 'ro-', markersize=3, label='Gradient Descent Path')\n",
          "\n",
          "# Label the plot\n",
          "plt.xlabel('x')\n",
          "plt.ylabel('y')\n",
          "plt.title('Gradient Descent on the Himmelblau Function')\n",
          "plt.legend()\n",
          "plt.colorbar(contour_plot)\n",
          "\n",
          "# Display the plot\n",
          "plt.show()"
        ]
      }
    ],
    "metadata": {
      "language_info": {
        "name": "python"
      }
    },
    "nbformat": 4,
    "nbformat_minor": 2
  }
  