{
    "cells": [
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "# Homework 0\n",
          "\n",
          "**Name:** -- Víctor Manuel Mariscal Cervantes --\n",
          "\n",
          "**e-mail:** -- victor.mariscal4459@alumnos.udg.mx --"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "# MODULES"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Load modules\n",
          "import numpy as np  # For numerical computations\n",
          "import matplotlib.pyplot as plt  # For plotting results\n",
          "import random  # For generating random numbers\n",
          "from matplotlib.colors import LogNorm  # For logarithmic normalization in contour plots\n",
          "\n",
          "# This magic command ensures that plots are displayed inline in the notebook\n",
          "%matplotlib inline"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "# Theory on the Gradient Descent Algorithm\n",
          "\n",
          "Gradient Descent is an iterative optimization algorithm used to find the minimum of a function. The algorithm works as follows:\n",
          "\n",
          "1. **Initialization:** Start with an initial guess for the parameters (e.g., a point in the function's domain).\n",
          "\n",
          "2. **Gradient Computation:** Calculate the gradient (i.e., the vector of partial derivatives) of the function at the current point. This gradient indicates the direction of steepest ascent.\n",
          "\n",
          "3. **Parameter Update:** Update the parameters by moving in the opposite direction of the gradient. The update rule is given by:\n",
          "\n",
          "$$\n",
          "\\mathbf{x}_{\\text{new}} = \\mathbf{x}_{\\text{current}} - \\alpha \\nabla f(\\mathbf{x}_{\\text{current}})\n",
          "$$\n",
          "\n",
          "where $\\alpha$ is the learning rate which determines the size of the step.\n",
          "\n",
          "4. **Convergence Check:** Repeat steps 2 and 3 until the change in parameters is less than a predefined tolerance, indicating that a minimum has been reached.\n",
          "\n",
          "In this notebook, we use the Himmelblau function, a well-known function with multiple local minima, to demonstrate gradient descent."
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## Himmelblau Function and Its Partial Derivatives\n",
          "\n",
          "The **Himmelblau function** is defined as:\n",
          "\n",
          "$$\n",
          "f(x,y) = (x^2 + y - 11)^2 + (x + y^2 - 7)^2\n",
          "$$\n",
          "\n",
          "This function has multiple local minima and is often used as a test function for optimization algorithms.\n",
          "\n",
          "The partial derivatives of the function are:\n",
          "\n",
          "$$ \\frac{\\partial f}{\\partial x} = 4x(x^2 + y - 11) + 2(x + y^2 - 7) $$\n",
          "\n",
          "$$ \\frac{\\partial f}{\\partial y} = 2(x^2 + y - 11) + 4y(x + y^2 - 7) $$\n",
          "\n",
          "These derivatives are used to update the parameters in the gradient descent algorithm."
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Function to be optimized\n",
          "def himmelblau(x, y):\n",
          "    r\"\"\"\n",
          "    Himmelblau function:\n",
          "    \n",
          "    Defined as:\n",
          "      f(x, y) = (x² + y - 11)² + (x + y² - 7)²\n",
          "    \n",
          "    Returns the function value at (x, y).\n",
          "    \"\"\"\n",
          "    return (x**2 + y - 11)**2 + (x + y**2 - 7)**2\n",
          "\n",
          "def grad_himmelblau(x, y):\n",
          "    r\"\"\"\n",
          "    Gradient of the Himmelblau function.\n",
          "    \n",
          "    The partial derivatives are:\n",
          "      \\( \\frac{\\partial f}{\\partial x} = 4x(x^2 + y - 11) + 2(x + y^2 - 7) \\)\n",
          "      \\( \\frac{\\partial f}{\\partial y} = 2(x^2 + y - 11) + 4y(x + y^2 - 7) \\)\n",
          "    \n",
          "    Returns a NumPy array with the derivatives with respect to x and y.\n",
          "    \"\"\"\n",
          "    dfdx = 4 * x * (x**2 + y - 11) + 2 * (x + y**2 - 7)\n",
          "    dfdy = 2 * (x**2 + y - 11) + 4 * y * (x + y**2 - 7)\n",
          "    return np.array([dfdx, dfdy])"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Run gradient descent algorithm\n",
          "\n",
          "# Set parameters for gradient descent\n",
          "learning_rate = 0.001       # Step size for each iteration\n",
          "max_iters = 10000           # Maximum number of iterations allowed\n",
          "tolerance = 1e-6            # Convergence tolerance\n",
          "\n",
          "# Choose a random initial point within the range [-6, 6] for both x and y\n",
          "x0 = random.uniform(-6, 6)\n",
          "y0 = random.uniform(-6, 6)\n",
          "x_current = np.array([x0, y0])\n",
          "print(\"Initial point:\", x_current)\n",
          "\n",
          "# List to store the path taken by the algorithm (for later visualization)\n",
          "path = [x_current.copy()]\n",
          "\n",
          "# Execute the gradient descent loop\n",
          "for i in range(max_iters):\n",
          "    # Compute the gradient at the current point\n",
          "    grad = grad_himmelblau(x_current[0], x_current[1])\n",
          "    \n",
          "    # Update the point by moving in the opposite direction of the gradient\n",
          "    x_new = x_current - learning_rate * grad\n",
          "    \n",
          "    # Append the new point to the path\n",
          "    path.append(x_new.copy())\n",
          "    \n",
          "    # Check for convergence: if the change in position is less than the tolerance, stop iterating\n",
          "    if np.linalg.norm(x_new - x_current) < tolerance:\n",
          "        print(f\"Convergence reached at iteration {i}\")\n",
          "        break\n",
          "    \n",
          "    # Update the current point for the next iteration\n",
          "    x_current = x_new\n",
          "\n",
          "# Convert the list of points to a NumPy array for easier manipulation\n",
          "path = np.array(path)\n",
          "\n",
          "# Output the final position and function value\n",
          "print(\"Final position:\", x_current)\n",
          "print(\"Final function value:\", himmelblau(x_current[0], x_current[1]))"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Plot the results (2D Contour Plot)\n",
          "\n",
          "# Create a grid of x and y values to evaluate the Himmelblau function\n",
          "x_vals = np.linspace(-6, 6, 400)\n",
          "y_vals = np.linspace(-6, 6, 400)\n",
          "X, Y = np.meshgrid(x_vals, y_vals)\n",
          "\n",
          "# Evaluate the Himmelblau function on the grid\n",
          "Z = himmelblau(X, Y)\n",
          "\n",
          "# Set up the plot\n",
          "plt.figure(figsize=(10, 6))\n",
          "\n",
          "# Plot the contour lines of the Himmelblau function\n",
          "contour_levels = np.logspace(-0.5, 5, 20)  # Logarithmically spaced levels\n",
          "contour_plot = plt.contour(X, Y, Z, levels=contour_levels, norm=LogNorm(), cmap='jet')\n",
          "plt.clabel(contour_plot, inline=True, fontsize=8)\n",
          "\n",
          "# Plot the path taken by the gradient descent algorithm\n",
          "plt.plot(path[:, 0], path[:, 1], 'ro-', markersize=3, label='Gradient Descent Path')\n",
          "\n",
          "# Label the plot\n",
          "plt.xlabel('x')\n",
          "plt.ylabel('y')\n",
          "plt.title('Gradient Descent on the Himmelblau Function')\n",
          "plt.legend()\n",
          "plt.colorbar(contour_plot)\n",
          "\n",
          "# Display the plot\n",
          "plt.show()"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# 3D Plot of the Himmelblau Function and Gradient Descent Path\n",
          "\n",
          "from mpl_toolkits.mplot3d import Axes3D  # Import for 3D plotting\n",
          "\n",
          "fig = plt.figure(figsize=(10, 6))\n",
          "ax = fig.add_subplot(111, projection='3d')\n",
          "\n",
          "# Plot the surface of the Himmelblau function\n",
          "surf = ax.plot_surface(X, Y, Z, cmap='jet', alpha=0.7)\n",
          "\n",
          "# Plot the gradient descent path in 3D\n",
          "ax.plot(path[:, 0], path[:, 1], himmelblau(path[:, 0], path[:, 1]), 'r.-', markersize=4, label='Gradient Descent Path')\n",
          "\n",
          "ax.set_xlabel('x')\n",
          "ax.set_ylabel('y')\n",
          "ax.set_zlabel('f(x, y)')\n",
          "ax.set_title('3D Plot of Himmelblau Function and Gradient Descent Path')\n",
          "ax.legend()\n",
          "plt.colorbar(surf, ax=ax, shrink=0.5, aspect=5)\n",
          "plt.show()"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## Conclusion\n",
          "\n",
          "In this notebook, I implemented the Gradient Descent algorithm to minimize the Himmelblau function. The process began by defining the Himmelblau function:\n",
          "\n",
          "$$ f(x,y) = (x^2 + y - 11)^2 + (x + y^2 - 7)^2 $$\n",
          "\n",
          "and its corresponding partial derivatives:\n",
          "\n",
          "$$ \\frac{\\partial f}{\\partial x} = 4x(x^2 + y - 11) + 2(x + y^2 - 7) $$\n",
          "$$ \\frac{\\partial f}{\\partial y} = 2(x^2 + y - 11) + 4y(x + y^2 - 7) $$\n",
          "\n",
          "I then initialized a random starting point in the domain $[-6, 6]\\times[-6, 6]$ and iteratively updated this point using the gradient descent update rule:\n",
          "\n",
          "$$ \\mathbf{x}_{\\text{new}} = \\mathbf{x}_{\\text{current}} - \\alpha \\nabla f(\\mathbf{x}_{\\text{current}}) $$\n",
          "\n",
          "with a learning rate $\\alpha = 0.001$. The iterations continued until the change in the position was less than the tolerance of $1\\times10^{-6}$ or until a maximum of 10,000 iterations was reached.\n",
          "\n",
          "Throughout the process, I recorded the path of the descent, which allowed me to visualize how the algorithm converged to a local minimum. I used both a 2D contour plot and a 3D surface plot to illustrate the function's landscape and the gradient descent path.\n",
          "\n",
          "This exercise helped me understand how gradient descent navigates complex, multi-modal functions like the Himmelblau function. By observing the descent path, I could see how the algorithm incrementally moved towards a local minimum despite the presence of multiple minima.\n",
          "\n",
          "Overall, I learned how to implement and document gradient descent in Python, utilize LaTeX for clear mathematical notation, and effectively visualize optimization processes. This hands-on experience deepened my understanding of iterative optimization techniques and their practical applications."
        ]
      }
    ],
    "metadata": {
      "language_info": {
        "name": "python"
      }
    },
    "nbformat": 4,
    "nbformat_minor": 2
  }
  