{
    "nbformat": 4,
    "nbformat_minor": 5,
    "metadata": {
      "name": "Reporte Detallado: Ejemplos de Cálculo de Descenso de Gradiente"
    },
    "cells": [
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "# **Reporte Detallado: Ejemplos de Cálculo de Descenso de Gradiente en Jupyter Notebook**\n",
          "\n",
          "A continuación, se presenta un *notebook* en formato de reporte que ilustra el **paso a paso** de cómo se aplica el algoritmo de **Descenso de Gradiente** para **cuatro funciones** diferenciables y convexas. Cada ejemplo:\n",
          "\n",
          "1. Elige un **punto inicial aleatorio** diferente cada vez que se ejecuta.\n",
          "2. Muestra cómo se realiza el **cálculo iterativo** hasta llegar (o acercarse) al **mínimo**.\n",
          "3. Genera **gráficas interactivas** (una para cada función) que te permitirán **hacer zoom, rotar y manipular** la figura con el mouse (utilizando `%matplotlib notebook`).\n",
          "\n",
          "> **Nota**: Para ejecutar este notebook con la visualización 3D interactiva, puede que necesites instalar la extensión `ipywidgets` o usar `%matplotlib widget`/`%matplotlib ipympl` según tu entorno.\n",
          "\n",
          "¡Por favor, copia y pega todo este contenido en un archivo `.ipynb` y ejecútalo para ver el reporte completo!\n"
        ]
      },
      {
        "cell_type": "code",
        "metadata": {
          "collapsed": false
        },
        "source": [
          "# --------------------------------------------------------------------------------\n",
          "# CELDA 1: Configuraciones iniciales y librerías\n",
          "# --------------------------------------------------------------------------------\n",
          "\n",
          "# Activamos el modo de plots interactivos para permitir zoom, rotación y manipulación en 3D.\n",
          "# NOTA: En algunos entornos, '%matplotlib notebook' puede requerir la extensión ipywidgets.\n",
          "# Si no funciona, podrías probar con '%matplotlib ipympl' o '%matplotlib widget'.\n",
          "%matplotlib notebook\n",
          "\n",
          "import numpy as np\n",
          "import matplotlib.pyplot as plt\n",
          "from mpl_toolkits.mplot3d import Axes3D  # Necesario para gráficos 3D interactivos\n",
          "import random\n",
          "\n",
          "# Fijar semilla comentado por defecto para que cada corrida sea distinta.\n",
          "# Si quieres ver siempre el mismo resultado, descomenta la siguiente línea:\n",
          "# np.random.seed(42)\n",
          "\n",
          "def grad_desc(\n",
          "    x: np.array,                # Punto inicial\n",
          "    f,                          # Función objetivo\n",
          "    gf,                         # Gradiente de la función objetivo\n",
          "    lr=0.05,                    # Tasa de aprendizaje\n",
          "    maxiter=20,                 # Número máximo de iteraciones\n",
          "    tol=1e-4                    # Tolerancia para la norma del gradiente\n",
          "):\n",
          "    \"\"\"\n",
          "    Esta función implementa el algoritmo de descenso por gradiente\n",
          "    y, además, va mostrando cada iteración.\n",
          "\n",
          "    Retorna:\n",
          "    - x: punto final tras las iteraciones\n",
          "    - historial: lista de puntos (np.array) visitados en cada iteración\n",
          "    \"\"\"\n",
          "    historial = [x.copy()]\n",
          "    \n",
          "    for i in range(maxiter):\n",
          "        grad = gf(x)\n",
          "        norm_grad = np.linalg.norm(grad)\n",
          "\n",
          "        # Imprimir información de la iteración\n",
          "        print(f\"Iteración {i+1}:\")\n",
          "        print(f\"  - Punto actual: {x}\")\n",
          "        print(f\"  - Valor de f(x): {f(x):.6f}\")\n",
          "        print(f\"  - Gradiente: {grad}\")\n",
          "        print(f\"  - Norma del gradiente: {norm_grad:.6f}\")\n",
          "        print(\"------------------------------------------------\")\n",
          "\n",
          "        # Verificamos si la norma del gradiente es menor a la tolerancia\n",
          "        if norm_grad < tol:\n",
          "            print(\"La norma del gradiente es menor que la tolerancia. Fin del descenso.\\n\")\n",
          "            break\n",
          "\n",
          "        # Actualizamos x usando la regla del descenso de gradiente\n",
          "        x = x - lr * grad\n",
          "        historial.append(x.copy())\n",
          "    \n",
          "    return x, historial\n",
          "\n",
          "def plot_3d_and_path(\n",
          "    f,              # Función objetivo de 2 variables\n",
          "    x_vals,         # Lista de puntos en el descenso de gradiente\n",
          "    title=\"Función\", \n",
          "    elev=20,        # Ángulo de elevación del gráfico 3D\n",
          "    azim=-60        # Ángulo azimutal (rotación en el plano XY)\n",
          "):\n",
          "    \"\"\"\n",
          "    Grafica la superficie 3D de la función 'f' (de R^2 en R)\n",
          "    y la ruta de puntos dada en x_vals.\n",
          "    \"\"\"\n",
          "    # Creación de la malla (X, Y) para graficar la superficie\n",
          "    grid_points = 100\n",
          "    rango = np.linspace(-5, 5, grid_points)\n",
          "    X, Y = np.meshgrid(rango, rango)\n",
          "    \n",
          "    # Calcular Z para toda la malla\n",
          "    Z = np.zeros_like(X)\n",
          "    for i in range(grid_points):\n",
          "        for j in range(grid_points):\n",
          "            Z[i, j] = f([X[i,j], Y[i,j]])\n",
          "    \n",
          "    # Figura en 3D\n",
          "    fig = plt.figure(figsize=(6, 5))\n",
          "    ax = fig.add_subplot(111, projection='3d')\n",
          "    ax.set_title(title)\n",
          "    \n",
          "    # Graficar la superficie\n",
          "    ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.7)\n",
          "\n",
          "    # Graficar la trayectoria del descenso de gradiente\n",
          "    x_path = [p[0] for p in x_vals]\n",
          "    y_path = [p[1] for p in x_vals]\n",
          "    z_path = [f(p) for p in x_vals]\n",
          "\n",
          "    ax.scatter(x_path, y_path, z_path, color='r', s=50, marker='o')\n",
          "    ax.plot(x_path, y_path, z_path, color='r', linewidth=2)\n",
          "\n",
          "    # Ajustar ángulos de cámara 3D\n",
          "    ax.view_init(elev=elev, azim=azim)\n",
          "\n",
          "    # Etiquetas de ejes\n",
          "    ax.set_xlabel('X')\n",
          "    ax.set_ylabel('Y')\n",
          "    ax.set_zlabel('f(X,Y)')\n",
          "    \n",
          "    plt.show()\n"
        ],
        "execution_count": null,
        "outputs": []
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## **Ejemplo 1: Función 1**\n",
          "\n",
          "La primera función es una de las más simples y conocidas:\n",
          "\n",
          "\\[\n",
          "f(x, y) = x^2 + y^2\n",
          "\\]\n",
          "\n",
          "- Es convexa.\n",
          "- Tiene un único mínimo global en \\((0, 0)\\).\n",
          "- Su gradiente es:\n",
          "\n",
          "\\[\n",
          "\\nabla f(x, y) = \\begin{pmatrix}\n",
          "2x \\\\\n",
          "2y\n",
          "\\end{pmatrix}.\n",
          "\\]\n",
          "\n",
          "A continuación, elegimos un **punto inicial aleatorio** y aplicamos el descenso de gradiente, mostrando en cada iteración:\n",
          "1. El punto actual.\n",
          "2. El valor de la función en ese punto.\n",
          "3. El gradiente y su norma.\n",
          "4. Verificamos si se cumple la tolerancia para detener el proceso.\n"
        ]
      },
      {
        "cell_type": "code",
        "metadata": {},
        "source": [
          "# --------------------------------------------------------------------------------\n",
          "# CELDA 2: Ejemplo 1\n",
          "# --------------------------------------------------------------------------------\n",
          "\n",
          "def f1(xy):\n",
          "    x, y = xy\n",
          "    return x**2 + y**2\n",
          "\n",
          "def grad_f1(xy):\n",
          "    x, y = xy\n",
          "    return np.array([2*x, 2*y])\n",
          "\n",
          "# Generamos un punto inicial aleatorio en el rango [-3,3]\n",
          "x0_1 = np.random.uniform(-3, 3, size=2)\n",
          "\n",
          "print(\"===== EJEMPLO 1: f(x,y) = x^2 + y^2 =====\")\n",
          "print(f\"Punto inicial aleatorio: {x0_1}\\n\")\n",
          "\n",
          "# Llamamos a la función de descenso de gradiente\n",
          "sol_1, historial_1 = grad_desc(x0_1, f1, grad_f1, lr=0.1, maxiter=15, tol=1e-4)\n",
          "\n",
          "print(\"\\nPunto encontrado como mínimo aproximado:\")\n",
          "print(f\"x* = {sol_1}\")\n",
          "print(f\"f(x*) = {f1(sol_1):.6f}\")\n",
          "\n",
          "# Graficamos en 3D la función y el recorrido\n",
          "plot_3d_and_path(f1, historial_1, title=\"Ejemplo 1: f(x,y) = x^2 + y^2\")\n"
        ],
        "execution_count": null,
        "outputs": []
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## **Ejemplo 2: Función 2**\n",
          "\n",
          "\\[\n",
          "f(x, y) = (x - 2)^2 + (y + 1)^2\n",
          "\\]\n",
          "\n",
          "- También es convexa.\n",
          "- Su mínimo está en \\((2, -1)\\).\n",
          "- El gradiente es:\n",
          "\n",
          "\\[\n",
          "\\nabla f(x, y) = \\begin{pmatrix}\n",
          "2(x - 2) \\\\\n",
          "2(y + 1)\n",
          "\\end{pmatrix}.\n",
          "\\]\n",
          "\n",
          "Seguiremos el mismo procedimiento (punto inicial aleatorio, iteraciones e impresión de resultados)."
        ]
      },
      {
        "cell_type": "code",
        "metadata": {},
        "source": [
          "# --------------------------------------------------------------------------------\n",
          "# CELDA 3: Ejemplo 2\n",
          "# --------------------------------------------------------------------------------\n",
          "\n",
          "def f2(xy):\n",
          "    x, y = xy\n",
          "    return (x - 2)**2 + (y + 1)**2\n",
          "\n",
          "def grad_f2(xy):\n",
          "    x, y = xy\n",
          "    return np.array([2*(x - 2), 2*(y + 1)])\n",
          "\n",
          "# Punto inicial aleatorio en el rango [-3,3]\n",
          "x0_2 = np.random.uniform(-3, 3, size=2)\n",
          "\n",
          "print(\"\\n===== EJEMPLO 2: f(x,y) = (x - 2)^2 + (y + 1)^2 =====\")\n",
          "print(f\"Punto inicial aleatorio: {x0_2}\\n\")\n",
          "\n",
          "sol_2, historial_2 = grad_desc(x0_2, f2, grad_f2, lr=0.1, maxiter=15, tol=1e-4)\n",
          "\n",
          "print(\"\\nPunto encontrado como mínimo aproximado:\")\n",
          "print(f\"x* = {sol_2}\")\n",
          "print(f\"f(x*) = {f2(sol_2):.6f}\")\n",
          "\n",
          "# Graficar\n",
          "plot_3d_and_path(f2, historial_2, title=\"Ejemplo 2: (x - 2)^2 + (y + 1)^2\")\n"
        ],
        "execution_count": null,
        "outputs": []
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## **Ejemplo 3: Función 3**\n",
          "\n",
          "\\[\n",
          "f(x, y) = x^2 + 2y^2\n",
          "\\]\n",
          "\n",
          "- Convexa.\n",
          "- Tiene su mínimo en \\((0, 0)\\).\n",
          "- Gradiente:\n",
          "\n",
          "\\[\n",
          "\\nabla f(x, y) = \\begin{pmatrix}\n",
          "2x \\\\\n",
          "4y\n",
          "\\end{pmatrix}.\n",
          "\\]\n",
          "\n",
          "Nuevamente se parte de un punto aleatorio y se observa la convergencia."
        ]
      },
      {
        "cell_type": "code",
        "metadata": {},
        "source": [
          "# --------------------------------------------------------------------------------\n",
          "# CELDA 4: Ejemplo 3\n",
          "# --------------------------------------------------------------------------------\n",
          "\n",
          "def f3(xy):\n",
          "    x, y = xy\n",
          "    return x**2 + 2*(y**2)\n",
          "\n",
          "def grad_f3(xy):\n",
          "    x, y = xy\n",
          "    return np.array([2*x, 4*y])\n",
          "\n",
          "# Punto inicial aleatorio\n",
          "x0_3 = np.random.uniform(-3, 3, size=2)\n",
          "\n",
          "print(\"\\n===== EJEMPLO 3: f(x,y) = x^2 + 2y^2 =====\")\n",
          "print(f\"Punto inicial aleatorio: {x0_3}\\n\")\n",
          "\n",
          "sol_3, historial_3 = grad_desc(x0_3, f3, grad_f3, lr=0.1, maxiter=15, tol=1e-4)\n",
          "\n",
          "print(\"\\nPunto encontrado como mínimo aproximado:\")\n",
          "print(f\"x* = {sol_3}\")\n",
          "print(f\"f(x*) = {f3(sol_3):.6f}\")\n",
          "\n",
          "# Graficar\n",
          "plot_3d_and_path(f3, historial_3, title=\"Ejemplo 3: x^2 + 2y^2\")\n"
        ],
        "execution_count": null,
        "outputs": []
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## **Ejemplo 4: Función 4**\n",
          "\n",
          "\\[\n",
          "f(x, y) = 3 (x - 1)^2 + 5 (y + 2)^2\n",
          "\\]\n",
          "\n",
          "- Convexa.\n",
          "- Su mínimo se encuentra en \\((1, -2)\\).\n",
          "- El gradiente:\n",
          "\n",
          "\\[\n",
          "\\nabla f(x, y) = \\begin{pmatrix}\n",
          "6(x - 1) \\\\\n",
          "10(y + 2)\n",
          "\\end{pmatrix}.\n",
          "\\]\n"
        ]
      },
      {
        "cell_type": "code",
        "metadata": {},
        "source": [
          "# --------------------------------------------------------------------------------\n",
          "# CELDA 5: Ejemplo 4\n",
          "# --------------------------------------------------------------------------------\n",
          "\n",
          "def f4(xy):\n",
          "    x, y = xy\n",
          "    return 3*(x - 1)**2 + 5*(y + 2)**2\n",
          "\n",
          "def grad_f4(xy):\n",
          "    x, y = xy\n",
          "    return np.array([6*(x - 1), 10*(y + 2)])\n",
          "\n",
          "# Punto inicial aleatorio\n",
          "x0_4 = np.random.uniform(-3, 3, size=2)\n",
          "\n",
          "print(\"\\n===== EJEMPLO 4: f(x,y) = 3 (x - 1)^2 + 5 (y + 2)^2 =====\")\n",
          "print(f\"Punto inicial aleatorio: {x0_4}\\n\")\n",
          "\n",
          "sol_4, historial_4 = grad_desc(x0_4, f4, grad_f4, lr=0.1, maxiter=15, tol=1e-4)\n",
          "\n",
          "print(\"\\nPunto encontrado como mínimo aproximado:\")\n",
          "print(f\"x* = {sol_4}\")\n",
          "print(f\"f(x*) = {f4(sol_4):.6f}\")\n",
          "\n",
          "# Graficar\n",
          "plot_3d_and_path(f4, historial_4, title=\"Ejemplo 4: 3(x-1)^2 + 5(y+2)^2\")\n"
        ],
        "execution_count": null,
        "outputs": []
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## **Conclusiones de cada paso**\n",
          "\n",
          "1. **Selección de punto aleatorio**: \n",
          "   - En cada ejemplo utilizamos `np.random.uniform(-3, 3, size=2)` para generar un vector \\((x, y)\\) aleatorio. Esto garantiza que cada ejecución comience en un punto inicial distinto dentro del cuadrado de coordenadas \\([-3, 3] \\times [-3, 3]\\).\n",
          "\n",
          "2. **Cálculo del gradiente**:\n",
          "   - Para cada función, se definió la **fórmula analítica** del gradiente. Este se evalúa en el punto actual en cada iteración.\n",
          "\n",
          "3. **Actualización de la posición**:\n",
          "   \\[\n",
          "   x_{\\text{nuevo}} = x_{\\text{viejo}} - \\alpha \\nabla f(x_{\\text{viejo}}),\n",
          "   \\]\n",
          "   - donde \\(\\alpha\\) (`lr`) es la **tasa de aprendizaje**.\n",
          "\n",
          "4. **Condición de parada**:\n",
          "   - El algoritmo se detiene cuando se supera un número máximo de iteraciones (`maxiter`) o cuando la norma del gradiente (`np.linalg.norm(grad)`) es menor que la **tolerancia** (`tol`).\n",
          "\n",
          "5. **Visualización 3D**:\n",
          "   - Cada figura se muestra de forma **interactiva**, lo que permite manipular la vista (rotar, hacer zoom) y observar de manera clara cómo el camino de descenso se dirige hacia el mínimo de la función.\n",
          "\n",
          "6. **Resultados**:\n",
          "   - Como se evidencia en los reportes impresos en cada iteración y en los gráficos 3D, las trayectorias convergen hacia los mínimos correspondientes.\n",
          "\n",
          "---\n",
          "### **Fin del Notebook**\n",
          "\n",
          "Cada vez que ejecutes este notebook de principio a fin, se generarán **cuatro puntos iniciales aleatorios** diferentes, por lo que **los caminos de descenso pueden variar** ligeramente. \n",
          "\n",
          "Si deseas **repetir** una ejecución exactamente, puedes **fijar la semilla** al principio (descomentando la línea `np.random.seed(42)` por ejemplo).\n",
          "\n",
          "Las funciones mostradas son todas **convexas**, razón por la cual el **descenso de gradiente** converge al **mínimo global** en cada caso.\n",
          "\n",
          "¡Disfruta explorando el **Descenso de Gradiente** con estos ejemplos!"
        ]
      }
    ]
  }
  